{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a1d1d3d-47b5-4490-970d-e238a933078f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_env_config(env: str):\n",
    "    env = (env or \"dev\").lower().strip()\n",
    "    if env not in [\"dev\", \"prod\"]:\n",
    "        env = \"dev\"\n",
    "\n",
    "    return {\n",
    "        \"ENV\": env,\n",
    "        \"CATALOG\": \"finance_data\",\n",
    "        \"SCHEMA\": env,\n",
    "        \"RAW_BUCKET\": \"gs://finance-datalake-raw\",\n",
    "        \"PROCESSED_BUCKET\": \"gs://finance-datalake-processed\",\n",
    "    }\n",
    "\n",
    "def tbl(cfg, name: str) -> str:\n",
    "    return f\"{cfg['CATALOG']}.{cfg['SCHEMA']}.{name}\"\n",
    "\n",
    "def path(cfg, layer: str, name: str) -> str:\n",
    "    # keep history separate per env\n",
    "    return f\"{cfg['PROCESSED_BUCKET']}/{cfg['ENV']}/{layer}/{name}/\"\n",
    "\n",
    "def checkpoint(cfg, name: str) -> str:\n",
    "    return f\"{cfg['PROCESSED_BUCKET']}/{cfg['ENV']}/checkpoints/{name}/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "252d15c5-7b4e-45c3-b3bc-2771b1c82abb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "RAW_INCOMING = \"gs://finance-datalake-raw/transactions/incoming/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53eb7b61-6a0e-45ea-8408-d1b566dfb7dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#RAW_BUCKET = \"gs://finance-datalake-raw\"\n",
    "#PROCESSED_BUCKET = \"gs://finance-datalake-processed\"\n",
    "\n",
    "#RAW_INCOMING = f\"{RAW_BUCKET}/transactions/incoming/\"\n",
    "#BRONZE_PATH = f\"{PROCESSED_BUCKET}/bronze/transactions/\"\n",
    "#SILVER_PATH = f\"{PROCESSED_BUCKET}/silver/transactions/\"\n",
    "#GOLD_PATH   = f\"{PROCESSED_BUCKET}/gold/\"\n",
    "\n",
    "#BRONZE_CHECKPOINT = f\"{PROCESSED_BUCKET}/checkpoints/bronze_transactions/\"\n",
    "#SILVER_CHECKPOINT = f\"{PROCESSED_BUCKET}/checkpoints/silver_transactions/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaa2b9d3-6fa2-43be-8b04-074fe19f3a50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def gcs_ls(path: str):\n",
    "    jvm = spark._jvm\n",
    "    conf = spark._jsc.hadoopConfiguration()\n",
    "    uri = jvm.java.net.URI(path)\n",
    "    fs = jvm.org.apache.hadoop.fs.FileSystem.get(uri, conf)\n",
    "    statuses = fs.listStatus(jvm.org.apache.hadoop.fs.Path(path))\n",
    "    return [s.getPath().toString() for s in statuses]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd0bab9b-c524-4b03-8fdc-18fe9b7bcfa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime, timezone\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "merchant_categories = [\"GROCERY\",\"FUEL\",\"RESTAURANT\",\"ONLINE_RETAIL\",\"UTILITIES\",\"TRAVEL\",\"PHARMACY\",\"TRANSFER\"]\n",
    "channels = [\"POS\",\"ONLINE\",\"ATM\"]\n",
    "cities = [\"Toronto\",\"Mississauga\",\"Brampton\",\"Ottawa\",\"Montreal\",\"Vancouver\",\"Calgary\",\"Edmonton\"]\n",
    "\n",
    "def make_batch(n=1000):\n",
    "    now = datetime.now(timezone.utc).isoformat()\n",
    "    rows = []\n",
    "    for _ in range(n):\n",
    "        amt = round(random.uniform(2, 5000), 2)\n",
    "        if random.random() < 0.02:\n",
    "            amt = round(random.uniform(7000, 25000), 2)\n",
    "\n",
    "        rows.append(Row(\n",
    "            transaction_id=f\"TXN{random.randint(10**11, 10**12-1)}\",\n",
    "            account_id=f\"ACCT{random.randint(10000, 99999)}\",\n",
    "            transaction_timestamp=now,\n",
    "            transaction_type=\"debit\" if random.random() < 0.85 else \"credit\",\n",
    "            amount=amt,\n",
    "            merchant_category=random.choice(merchant_categories),\n",
    "            channel=random.choice(channels),\n",
    "            city=random.choice(cities)\n",
    "        ))\n",
    "    return spark.createDataFrame(rows)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_config_and_helpers",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
